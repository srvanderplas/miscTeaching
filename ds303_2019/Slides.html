<!DOCTYPE html>
<html>
  <head>
    <title>Visual Diagnostics for Statistical Modeling</title>
    <meta charset="utf-8">
    <meta name="date" content="2018-09-16" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Visual Diagnostics for Statistical Modeling
### 2018-09-16

---





class: inverse,middle,center
# Randomness, Models, and Errors



---
class: middle

- Monday: Slides 1-37


- Wednesday: [Slide 38](#wednesday-start)-59

- Friday: [Slide 60](#friday-start) - end

---
class: middle,center
### In the beginning, there was the data...

---

![](Slides_files/figure-html/init-olympic-1.png)&lt;!-- --&gt;
--
and the statistician said "Let there be a model". 

---

![](Slides_files/figure-html/init-olympic-lm-1.png)&lt;!-- --&gt;

And we fit a linear model. 

---

![](Slides_files/figure-html/init-olympic-errors-1.png)&lt;!-- --&gt;

The model separated the data into predictions and errors, 

---

![](Slides_files/figure-html/init-olympic-known-unknown-1.png)&lt;!-- --&gt;

or "known" and "unknown" sources of variation. 
---
class: middle,center

The statistician called the "unknown" variation __random error__,    
but s/he did not rest. 
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;
--
[Variance Decomposition Derivation](https://en.wikipedia.org/wiki/Variance#Decomposition)


---
## Randomness and Error

Linear regression model form: `$$y = w_0 + w_1 x$$`

--

This model is __deterministic__ - you will always get the same `\(y\)` value when you have multiple observations at the same `\(x\)`

&lt;br/&gt;&lt;br/&gt;

--

Probabilistic version: `$$y = w_0 + w_1 x + \epsilon$$`

`\(\epsilon\)` represents "random error"

A __random variable__ is a variable that can take on many values with different probabilities. `\(x\)`, `\(y\)`, and `\(\epsilon\)` are random variables. 

---
## Sources of Error

- measurement
- environmental
- human
- modeling

We group all of these into "random error" and model it with `\(\epsilon\)`. 

![Calvin and Hobbes on Surveys](calvin_hobbes_messing_with_data.png)

---
## Random Variables

Random variables have a __distribution__ of potential values.

![](Slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---
## Statistical Modeling

We usually assume that errors are

- independent (of other errors)

- identically distributed (all errors come from the same distribution)

- (sometimes) errors come from a specific distribution

--

... that's where the trouble starts...

---
## Statistical Modeling

Most of the trouble in modeling and prediction comes from three places:

- non-independent errors    
e.g. relationships between points within the data set

- non identical errors    
e.g. systematic variation not accounted for by the model

- mismatches between model assumptions and the distribution of the errors

---
class: middle, center, inverse
# Visual Diagnostics for Models


---
## Visual Diagnostics for Models

For some model `$$y = f(\vec{x}) + \epsilon$$`

- How do we examine our errors for problems?

- How do we determine what model (of several) fits the data best?

---

## Exploratory Data Analysis

Before you start modeling

- Plot `\(y\)` against any explanatory variables `\(x_1, ..., x_n\)`

- Plot the distribution of `\(y\)`, the distribution of `\(x_1, ..., x_n\)` to look for unexpected features

- Plot the relationship between different `\(x_i\)`'s

???

Visualization is an art as well as a science, and requires thinking critically about your data while being open to finding new things.

---

## United Nations Human Development Data


The United Nations maintains a dataset with quality-of-life indicators for most countries in the world. You can explore the data [here](http://hdr.undp.org/en/data). 

There is also a "World Happiness Index" ([link](https://countryeconomy.com/demography/world-happiness-index))

Let's predict happiness based on factors in the UN Quality of Life dataset


```r
library(tidyverse)
# csv url: https://raw.githubusercontent.com/srvanderplas/
#          miscTeaching/master/ds303_2019/2017_Human_Dev_Index.csv

hdi_2017 &lt;- read_csv("https://bit.ly/2mbYdfX")

hdi_2017
```

```
## # A tibble: 154 x 74
##    Country Happiness_Rank Happiness_Index  year Coef_Human_Ineq…
##    &lt;chr&gt;            &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;
##  1 United…             19            6.89  2017            13.1 
##  2 United…             15            7.05  2017             9.09
##  3 Germany             17            6.98  2017             7.79
##  4 France              24            6.59  2017            10.1 
##  5 Japan               58            5.89  2017             3.59
##  6 Spain               30            6.35  2017            14.9 
##  7 Italy               36            6.22  2017            11.9 
##  8 Portug…             66            5.69  2017            13.2 
##  9 Greece              82            5.29  2017            13.1 
## 10 Ireland             16            7.02  2017             8.60
## # … with 144 more rows, and 69 more variables:
## #   Elderly_Dependency_Pct &lt;dbl&gt;, Empl_Agriculture &lt;dbl&gt;,
## #   Empl_Nonagriculture_Pct_Female &lt;dbl&gt;, Empl_Pop_Ratio &lt;dbl&gt;,
## #   Empl_Services &lt;dbl&gt;, Empl_Vulnerable_Pct &lt;dbl&gt;,
## #   Expected_years_of_schooling &lt;dbl&gt;, Exports_Imports_PctGDP &lt;dbl&gt;,
## #   Female_Bank_Acct_Pct &lt;dbl&gt;,
## #   Finance_Sector_Domestic_Credit_PctGDP &lt;dbl&gt;,
## #   Foreign_Direct_Investment_PctGDP &lt;dbl&gt;, GDP &lt;dbl&gt;, GDP_pc &lt;dbl&gt;,
## #   GNIpc_female &lt;dbl&gt;, GNIpc_male &lt;dbl&gt;, Gross_Capital_PctGDP &lt;dbl&gt;,
## #   Gross_Fixed_Capital_PctGDP &lt;dbl&gt;, Gross_National_Income &lt;dbl&gt;,
## #   HDI_rank &lt;dbl&gt;, Idx_Education &lt;dbl&gt;, Idx_Gender_Development &lt;dbl&gt;,
## #   Idx_Gender_Inequality &lt;dbl&gt;, Idx_Human_Development &lt;dbl&gt;,
## #   Idx_Human_Development_female &lt;dbl&gt;, Idx_Human_Development_male &lt;dbl&gt;,
## #   Idx_Income &lt;dbl&gt;, Idx_IneqAdj_education &lt;dbl&gt;,
## #   Idx_IneqAdj_income &lt;dbl&gt;, Idx_IneqAdj_Life_Exp &lt;dbl&gt;,
## #   Idx_Red_List &lt;dbl&gt;, Ineq_education &lt;dbl&gt;, Ineq_HDI_Loss &lt;dbl&gt;,
## #   Ineq_income &lt;dbl&gt;, Ineq_Life_Exp &lt;dbl&gt;,
## #   `Inequality-adjusted_HDI` &lt;dbl&gt;, Infants_Missing_DPT_Vaccine &lt;dbl&gt;,
## #   Infants_Missing_measles_Vaccine &lt;dbl&gt;, Labor_Participation &lt;dbl&gt;,
## #   Labor_Participation_female &lt;dbl&gt;, Labor_Participation_male &lt;dbl&gt;,
## #   Life_Exp_Birth &lt;dbl&gt;, Life_Exp_Birth_female &lt;dbl&gt;,
## #   Life_Exp_Birth_male &lt;dbl&gt;, Life_Exp_index &lt;dbl&gt;,
## #   MtoF_Ratio_birth &lt;dbl&gt;, Parlaiment_Pct_Female &lt;dbl&gt;,
## #   `Pop_15–64` &lt;dbl&gt;, Pop_65 &lt;dbl&gt;, Pop_Total &lt;dbl&gt;, Pop_under5 &lt;dbl&gt;,
## #   Pop_Urban_Pct &lt;dbl&gt;, Private_capital_flows_PctGDP &lt;dbl&gt;,
## #   Refugees_from_country_1000s &lt;dbl&gt;, Remittances_inflows_PctGDP &lt;dbl&gt;,
## #   School_Expected_Years_female &lt;dbl&gt;, School_Expected_Years_male &lt;dbl&gt;,
## #   School_Mean_Years &lt;dbl&gt;, School_Mean_Years_female &lt;dbl&gt;,
## #   School_Mean_Years_male &lt;dbl&gt;, School_Secondary_Pop_Pct &lt;dbl&gt;,
## #   School_Secondary_Pop_Pct_female &lt;dbl&gt;,
## #   School_Secondary_Pop_Pct_male &lt;dbl&gt;, Teen_Birth_Rate &lt;dbl&gt;,
## #   Unemployment_FtoM_Ratio &lt;dbl&gt;, Unemployment_total &lt;dbl&gt;,
## #   Unemployment_youth &lt;dbl&gt;, Working_Poor_Pct_3.10day &lt;dbl&gt;,
## #   Young_Dependency_Pct &lt;dbl&gt;, Youth_Unemployment_FtoM &lt;dbl&gt;
```

---
## Exploratory Data Analysis

What variables might be relevant to the Happiness Index on a national scale?


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = GDP, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;


---
## Exploratory Data Analysis

What variables might be relevant to the Happiness Index on a national scale?


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = GDP, y = Happiness_Index)) + 
  xlim(c(0, 2500))
```

![](Slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = GDP, y = Happiness_Index)) + 
  scale_x_log10()
```

![](Slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
`\(\log_{10}(GDP)\)`

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Gross_National_Income, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

Might not be linearly related, but definitely related...

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Life_Exp_Birth, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

Again, possible nonlinearity, but definitely related

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Parlaiment_Pct_Female, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Teen_Birth_Rate, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Ineq_education, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
Inequality in Education(%) : Inequality in distribution of years of schooling based on data from household surveys... [Source](http://hdr.undp.org/en/composite/IHDI#a)

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Ineq_income, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---
## Exploratory Data Analysis


```r
ggplot(hdi_2017) + 
  geom_point(aes(x = Ineq_Life_Exp, y = Happiness_Index))
```

![](Slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---
## Exploratory Data Analysis


```r
model_data &lt;- select(hdi_2017, Country, Happiness_Index, 
                     Ineq_education, Ineq_income, Ineq_Life_Exp, 
                     Teen_Birth_Rate, Life_Exp_Birth, 
                     GDP, Gross_National_Income) %&gt;%
  mutate(log10GDP = log10(GDP)) %&gt;%
  select(-GDP) %&gt;%
  na.omit()

install.packages("GGally")
library(GGally)
ggscatmat(data = model_data[, -c(1:2)]) 
# Scatterplot Matrix - see all of the independent variables and how they relate
# to each other
```


---
## Exploratory Data Analysis

&lt;img src="Slides_files/figure-html/unnamed-chunk-15-1.png" width="60%" /&gt;

---
## Fit a model


```r
bad_model &lt;- lm(Happiness_Index ~ ., data = select(model_data, -Country))
summary(bad_model)
```

```
## 
## Call:
## lm(formula = Happiness_Index ~ ., data = select(model_data, -Country))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.82664 -0.36725  0.06825  0.38399  1.32214 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           -1.703e+00  1.843e+00  -0.924 0.357298    
## Ineq_education        -1.268e-02  6.499e-03  -1.951 0.053211 .  
## Ineq_income            3.504e-04  5.984e-03   0.059 0.953389    
## Ineq_Life_Exp          1.694e-02  1.935e-02   0.875 0.382940    
## Teen_Birth_Rate        5.491e-03  2.243e-03   2.448 0.015708 *  
## Life_Exp_Birth         8.673e-02  2.299e-02   3.772 0.000244 ***
## Gross_National_Income  2.848e-05  5.070e-06   5.617 1.12e-07 ***
## log10GDP               2.467e-02  7.712e-02   0.320 0.749538    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6257 on 131 degrees of freedom
## Multiple R-squared:  0.7088,	Adjusted R-squared:  0.6932 
## F-statistic: 45.54 on 7 and 131 DF,  p-value: &lt; 2.2e-16
```

```r
model_data$bad_model_resid &lt;- residuals(bad_model)
model_data$bad_model_predictions &lt;- predict(bad_model, newdata = model_data)
```

---
## Model Diagnostics


```r
n_density &lt;- tibble(x = seq(-3, 3, .01), y = dnorm(x))
ggplot(model_data, aes(x = bad_model_resid)) + 
  geom_histogram(aes(y = ..density..), binwidth = .2)
```

![](Slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---
## Model Diagnostics


```r
ggplot(model_data, aes(sample = bad_model_resid)) + 
  geom_qq() + 
  geom_qq_line()
```

![](Slides_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---
## Model Diagnostics


```r
ggplot(data = model_data, aes(x = Happiness_Index, y = bad_model_predictions)) + 
  geom_point() + geom_abline(slope = 1, intercept = 0)
```

![](Slides_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---
## Model Diagnostics


```r
ggplot(data = model_data, aes(x = Happiness_Index, y = bad_model_resid)) + 
  geom_point() + geom_smooth(method = "lm")
```

![](Slides_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---
## Model Diagnostics


```r
ggplot(data = model_data, aes(x = cut_number(Happiness_Index, 8), y = bad_model_resid)) + 
  geom_jitter() + 
  geom_boxplot(alpha = .5)
```

![](Slides_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;



---
## Model Diagnostics


```r
model_data$bad_model_resid &lt;- residuals(bad_model)

ggplot(data = model_data, aes(x = Gross_National_Income, y = bad_model_resid)) + 
  geom_point() + geom_smooth(method = "lm")
```

![](Slides_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;



---
name:wednesday-start
## Get set up


```r
# Get essential code from monday's class - 
#   - read in data
#   - fit bad model
#   - save bad model predictions/residuals
source("http://bit.ly/ds303-monday")
```

```
## Parsed with column specification:
## cols(
##   .default = col_double(),
##   Country = col_character()
## )
```

```
## See spec(...) for full column specifications.
```

---
## Fit another model

```r
meh_model &lt;- lm(Happiness_Index ~ ., 
                data = select(model_data, -Country, 
                              -Gross_National_Income, 
                              -matches("bad_model")))
summary(meh_model)
```

```
## 
## Call:
## lm(formula = Happiness_Index ~ ., data = select(model_data, -Country, 
##     -Gross_National_Income, -matches("bad_model")))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.79846 -0.43521  0.01812  0.48450  1.63138 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -4.2539441  1.9820346  -2.146  0.03368 *  
## Ineq_education  -0.0202762  0.0070539  -2.874  0.00472 ** 
## Ineq_income     -0.0007479  0.0066368  -0.113  0.91045    
## Ineq_Life_Exp    0.0301047  0.0213121   1.413  0.16014    
## Teen_Birth_Rate  0.0037366  0.0024653   1.516  0.13199    
## Life_Exp_Birth   0.1277461  0.0241912   5.281 5.16e-07 ***
## log10GDP         0.0903391  0.0845904   1.068  0.28749    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6944 on 132 degrees of freedom
## Multiple R-squared:  0.6386,	Adjusted R-squared:  0.6222 
## F-statistic: 38.88 on 6 and 132 DF,  p-value: &lt; 2.2e-16
```

```r
model_data$meh_model_resid &lt;- resid(meh_model)
```

---
## Model Diagnostics


```r
ggplot(data = model_data, 
       aes(x = Happiness_Index, y = meh_model_resid)) + 
  geom_point() + geom_smooth(method = "lm")
```

![](Slides_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---
## Model Comparison


```r
ggplot(data = model_data, 
       aes(x = meh_model_resid, y = bad_model_resid)) + 
  geom_point() + geom_smooth(method = "lm")
```

![](Slides_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
Removing a significant variable from the model and nothing changes...

---
## Model Diagnostics


```r
ggplot(data = model_data, 
       aes(x = Ineq_education, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;

---
## Model Diagnostics

```r
ggplot(data = model_data, 
       aes(x = Ineq_income, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;

---
## Model Diagnostics

```r
ggplot(data = model_data, 
       aes(x = Ineq_Life_Exp, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;


---
## Model Diagnostics

```r
ggplot(data = model_data, 
       aes(x = Teen_Birth_Rate, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;


---
## Model Diagnostics

```r
ggplot(data = model_data, 
       aes(x = Life_Exp_Birth, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;



---
## Model Diagnostics

```r
ggplot(data = model_data, 
       aes(x = log10GDP, y = meh_model_resid)) + 
  geom_point() + 
  geom_smooth()
```

![](Slides_files/figure-html/unnamed-chunk-32-1.png)&lt;!-- --&gt;

---
## Generalized Linear Model Diagnostics - Leverage


```r
library(car)
leveragePlots(meh_model, layout = c(2, 3))
```

&lt;img src="Slides_files/figure-html/unnamed-chunk-34-1.png" width="100%" /&gt;
High-Leverage points have a large potential to affect the results of an analysis. They are not always visible from residual plots. 

---
## Generalized Linear Model Diagnostics - Leverage
&lt;br/&gt;&lt;br/&gt;
`$$\text{leverage at }x_i = h_{ii} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_i(x_i - \bar{x})^2}$$`

Leverage is mostly important in functional regression models; it has relatively small effects in ensemble models such as random forests.

---
## Generalized Linear Model Diagnostics - Influence


```r
cutoff &lt;- 4/((nrow(model_data) - length(meh_model$coefficients) - 2))
plot(meh_model, which = 4, cook.levels = cutoff)
```

![](Slides_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;

Cook's distance identifies points which are too influential.

---
## Generalized Linear Model Diagnostics - Influence


```r
res &lt;- influencePlot(meh_model, id.method = "identify", main = "Influence Plot", 
                     sub = "Circle size is proportial to Cook's Distance")
```

![](Slides_files/figure-html/unnamed-chunk-36-1.png)&lt;!-- --&gt;

---
## Generalized Linear Model Diagnostics - Influence

Extra reading: 

- [Studentized residuals](https://en.wikipedia.org/wiki/Studentized_residual) - residuals from leave-one-out cross-validation     
(leave out that point, fit the model, predict for the left-out point, calculate the residual)

- [Hat-values](https://en.wikipedia.org/wiki/Projection_matrix) - influence each response value has on the fitted value

- [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance): the sum of all the changes in the regression model when observation `\(i\)` is removed `$$D_i = \frac{\sum_{j=1}^n (\hat{y}_j - \hat{y}_{j(i)})^2}{ps^2}$$` where p is the number of parameters and `\(s^2\)` is the mean squared error of the regression model

---
## Generalized Linear Model Diagnostics - Influence


```r
# Look at high influence or high leverage points
model_data[c(23, 34, 47, 72, 97, 106, 130, 137), 
           c("Country", "Happiness_Index", "meh_model_resid")]
```

```
## # A tibble: 8 x 3
##   Country      Happiness_Index meh_model_resid
##   &lt;chr&gt;                  &lt;dbl&gt;           &lt;dbl&gt;
## 1 Burundi                 3.78          0.127 
## 2 Ivory Coast             4.94          1.57  
## 3 Egypt                   4.17         -0.767 
## 4 Comoros                 3.97         -0.0500
## 5 Nigeria                 5.26          1.63  
## 6 Pakistan                5.65          0.999 
## 7 Tanzania                3.23         -1.80  
## 8 South Africa            4.72          0.180
```
(Comoros is an island nation off of the coast of eastern Africa. )



---
## Model Diagnostics

Residual-based diagnostics work for pretty much any type of model that generates predictions.


```r
library(randomForest)
rf_model &lt;- randomForest(Happiness_Index ~ ., 
                         data = select(model_data, -Country))
print(rf_model)
model_data$rf_predict &lt;- predict(rf_model)
model_data$rf_resid &lt;- model_data$Happiness_Index - 
  model_data$rf_predict
```



You'll talk about random forests later in the semester. They're not a linear model, so they work a bit differently.

---
## Model Diagnostics


```r
ggplot(data = model_data, 
       aes(x = Happiness_Index, y = rf_predict)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0)
```

![](Slides_files/figure-html/unnamed-chunk-40-1.png)&lt;!-- --&gt;


---
## Model Diagnostics


```r
ggplot(data = model_data, 
       aes(x = Happiness_Index, y = rf_resid)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

![](Slides_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;

---
## Model Diagnostics


```r
ggplot(data = model_data, aes(x = rf_resid)) + 
  geom_histogram()
```

![](Slides_files/figure-html/unnamed-chunk-42-1.png)&lt;!-- --&gt;


---
## Model Comparison


```r
ggplot(data = model_data, 
       aes(x = meh_model_resid, y = rf_resid)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  coord_fixed(ratio = 1) + 
  geom_smooth(se = F)
```

![](Slides_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;

---
## Your Turn

- Work in groups of 3-4
- Pick 1 dataset:
  - Education-related variables (http://bit.ly/ds303_edu)
  - GDP-related variables (http://bit.ly/ds303_gdp)
  - other Indexes as variables (http://bit.ly/ds303_idx)
  - Medical and Poverty-related variables (http://bit.ly/ds303_med)

- Each dataset has predicted values from 3 models: 
  - a knn model
  - a linear model
  - a random forest

- Compare the models, identify areas where each model doesn't fit, and (if you have time) find other variables (columns after Country in the csv) which might be important to include


---
class:middle,inverse,center
name: friday-start
## Visualizations for Cross-Validation 

---
## Tidy models

- A unified framework for modeling in R

- Goal is to have the same (tidy) output for common models to facilitate comparisons

- `parsnip`: tidy interface for models,

- `broom`: tidy format for model analysis objects, 

- `yardstick`: tidy measures of model performance

- `rsample`: tidy ways to resample data (cross-validation, etc.)


```r
library(tidyverse)
install.packages(c("parsnip", "broom", "yardstick", 
                   "rsample", "tidymodels", "AmesHousing"))
```

---
## The `parsnip` package


```r
library(tidyverse)
library(tidymodels)
library(AmesHousing)
ames &lt;- make_ames()
set.seed(4595)
# Split data into training and test set
data_split &lt;- initial_split(ames, strata = "Sale_Price", p = 0.9)
ames_train &lt;- training(data_split)
ames_test  &lt;- testing(data_split)
```

---
## Parsnip


```r
rf_model &lt;- rand_forest(mode = "regression") %&gt;% 
  set_engine("ranger") # set_engine specifies 
                       # how the model is fit
linear_model &lt;- linear_reg() %&gt;% 
  set_engine("lm")

models &lt;- tibble::tibble(
  model_name = c("random_forest", "linear_regression"), 
  model_fcn = list(rf_model, linear_model)) %&gt;%
  dplyr::mutate(
    # fit using the training set
    res = purrr::map(
      model_fcn, 
      ~fit(., formula = log10(Sale_Price) ~ 
             Longitude + Latitude + 
             Lot_Area + Neighborhood + 
             Year_Sold, data = ames_train)),
    # predict based on the test set
    pred = purrr::map(res, predict, 
                      new_data = ames_test)
  )
```

---
## Parsnip

```r
models$res[[1]]
```

```
## parsnip model object
## 
## Ranger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      2639 
## Number of independent variables:  5 
## Mtry:                             2 
## Target node size:                 5 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.00836771 
## R squared (OOB):                  0.7352824
```

---
## Parsnip

```r
models$res[[2]]
```

```
## parsnip model object
## 
## 
## Call:
## stats::lm(formula = formula, data = data)
## 
## Coefficients:
##                                         (Intercept)  
##                                          -1.520e+02  
##                                           Longitude  
##                                          -1.363e+00  
##                                            Latitude  
##                                           8.168e-01  
##                                            Lot_Area  
##                                           3.148e-06  
##                           NeighborhoodCollege_Creek  
##                                           5.970e-02  
##                                NeighborhoodOld_Town  
##                                          -6.793e-02  
##                                 NeighborhoodEdwards  
##                                          -1.089e-01  
##                                NeighborhoodSomerset  
##                                           1.595e-01  
##                      NeighborhoodNorthridge_Heights  
##                                           2.721e-01  
##                                 NeighborhoodGilbert  
##                                           7.492e-02  
##                                  NeighborhoodSawyer  
##                                          -8.944e-02  
##                          NeighborhoodNorthwest_Ames  
##                                           8.449e-02  
##                             NeighborhoodSawyer_West  
##                                           6.643e-03  
##                                NeighborhoodMitchell  
##                                           9.704e-02  
##                               NeighborhoodBrookside  
##                                          -6.057e-02  
##                                NeighborhoodCrawford  
##                                           1.321e-01  
##                  NeighborhoodIowa_DOT_and_Rail_Road  
##                                          -1.505e-01  
##                              NeighborhoodTimberland  
##                                           1.934e-01  
##                              NeighborhoodNorthridge  
##                                           2.907e-01  
##                             NeighborhoodStone_Brook  
##                                           2.846e-01  
## NeighborhoodSouth_and_West_of_Iowa_State_University  
##                                          -4.738e-02  
##                             NeighborhoodClear_Creek  
##                                           4.196e-02  
##                          NeighborhoodMeadow_Village  
##                                          -9.550e-02  
##                               NeighborhoodBriardale  
##                                          -1.303e-01  
##                     NeighborhoodBloomington_Heights  
##                                           1.098e-01  
##                                 NeighborhoodVeenker  
##                                           1.663e-01  
##                         NeighborhoodNorthpark_Villa  
##                                           3.361e-03  
##                                 NeighborhoodBlueste  
##                                           1.163e-02  
##                                  NeighborhoodGreens  
##                                           1.097e-01  
##                             NeighborhoodGreen_Hills  
##                                           2.065e-01  
##                                NeighborhoodLandmark  
##                                          -6.974e-02  
##                                           Year_Sold  
##                                          -2.436e-03
```

---
## Parsnip - Model Comparisons


```r
ames_test_preds &lt;- models %&gt;%
  # Get predictions and model names
  select(model_name, pred) %&gt;% 
  # Make a long, skinny data frame
  unnest() %&gt;% 
  # rename variables
  rename(pred = .pred) %&gt;% 
  # For each model, 
  group_by(model_name) %&gt;% 
  # number the rows 1:n(), where n() = 731
  mutate(row = 1:n()) %&gt;%  
  # Put each model in a different column
  spread(key = model_name, value = pred) %&gt;% 
  # Add in the actual test set data
  bind_cols(ames_test) %&gt;% 
  # Go back to a long-form dataset where predictions are in one column
  gather(key = "model_name", value = "pred", 
         random_forest:linear_regression)
```

---
## Model Comparisons


```r
ggplot(aes(x = log10(Sale_Price), y = pred), 
       data = ames_test_preds) + 
  geom_point() + facet_wrap(~model_name) + 
  coord_fixed() + 
  ggtitle("Test Set Predictions")
```

&lt;img src="Slides_files/figure-html/unnamed-chunk-50-1.png" width="100%" /&gt;



---

## Cross-Validation


```r
ames_train_1var &lt;- select(ames_train, Sale_Price, Gr_Liv_Area)
ames_cv &lt;- vfold_cv(ames_train_1var, v = 30, strata = "Sale_Price", breaks = 8)
ames_cv[1,]
```

```
## # A tibble: 1 x 2
##   splits            id    
## * &lt;named list&gt;      &lt;chr&gt; 
## 1 &lt;split [2.5K/92]&gt; Fold01
```

```r
ames_cv$splits[[1]]
```

```
## &lt;2547/92/2639&gt;
```

---
## Cross-Validation

```r
analysis(ames_cv$splits[[1]])
```

```
## # A tibble: 2,547 x 2
##    Sale_Price Gr_Liv_Area
##         &lt;int&gt;       &lt;int&gt;
##  1     215000        1656
##  2     172000        1329
##  3     244000        2110
##  4     189900        1629
##  5     195500        1604
##  6     213500        1338
##  7     191500        1280
##  8     236500        1616
##  9     189000        1804
## 10     175900        1655
## # … with 2,537 more rows
```

```r
assessment(ames_cv$splits[[1]])
```

```
## # A tibble: 92 x 2
##    Sale_Price Gr_Liv_Area
##         &lt;int&gt;       &lt;int&gt;
##  1     149900        1078
##  2     230000        1492
##  3     172500        1556
##  4     242000        2978
##  5     236000        1949
##  6     318000        1978
##  7      90000         960
##  8      55993        1044
##  9     246000        1646
## 10     128000        1180
## # … with 82 more rows
```

---

## Cross-Validation


```r
# Create our model fitting function
ames_lm &lt;- function(cv) {
  lm(formula = log10(Sale_Price) ~ Gr_Liv_Area,
     data = as.data.frame(cv))
}

ames_cv_lm &lt;- ames_cv %&gt;% 
  # map takes each set and applies a function
  mutate(model = purrr::map(splits, ames_lm)) %&gt;%
  # get out the coefficients
  mutate(coefs = purrr::map(model, "coefficients"),
         slope = purrr::map_dbl(coefs, "Gr_Liv_Area"),
         intercept = purrr::map_dbl(coefs, "(Intercept)"))
```

---
## Cross-Validation - Parameter Estimate Error


```r
ggplot(data = ames_cv_lm) + 
  geom_histogram(aes(x = slope)) + 
  ggtitle("Cross-Validated Slope Values")
```

&lt;img src="Slides_files/figure-html/unnamed-chunk-55-1.png" width="50%" /&gt;

---
## Cross-Validation


```r
ames_loess &lt;- function(cv) {
  loess(Sale_Price ~ Gr_Liv_Area, 
        data = analysis(cv))
}

# Make a table of possible predictive values
output_range &lt;- tibble(
  Gr_Liv_Area = min(ames$Gr_Liv_Area):max(ames$Gr_Liv_Area)
) 

# Get regression lines and assessment set predictions
ames_cv_loess &lt;- ames_cv %&gt;% 
  mutate(
    model = purrr::map(splits, ames_loess),
    pred_line = purrr::map(model, ~bind_cols(
      output_range, 
      pred = predict(., newdata = output_range))),
    pred_assessment = purrr::map2(
      model, splits, ~bind_cols(
        assessment(.y), 
        pred = predict(.x, newdata = assessment(.y))))
  )
```

---
## Cross-Validation - Model Variability


```r
select(ames_cv_loess, id, pred_line) %&gt;% unnest() %&gt;%
  ggplot() + 
  geom_point(aes(x = Gr_Liv_Area, y = Sale_Price), 
             data = ames_train_1var, color = "red", alpha = .2) + 
  geom_line(aes(x = Gr_Liv_Area, y =  pred, group = id), alpha = .1) + 
  scale_y_log10(breaks = c(50000, 100000, 150000, 300000, 500000))
```

&lt;img src="Slides_files/figure-html/unnamed-chunk-57-1.png" width="100%" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
